"""
Name: Anik Singha
Email: anik.singha68@myhunter.cuny.edu
Resources:
"""

import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures

def import_data(csv_file):
    '''
    This function takes one input:
    csv_file: the name of a CSV file time series data for commodities from FRED.
    The data in the file is read into a DataFrame, and a new column, units, that indexes
    the lines of the files and represents the number of units (e.g. months, weeks, years).
    It can be generated by looping through the dataset or using the index as a column
    (e.g. df.index.to_series()). The resulting DataFrame is returned.
    '''
    df = pd.read_csv(csv_file)
    df["units"] = df.index.to_series()
    return df

def split_data(df, y_col_name, test_size = 0.25, random_state = 21):
    '''
    This function takes 4 input parameters:
    df: a DataFrame containing with a columns units.
    y_col_name: the name of the column of the dependent variable.
    test_size: accepts a float between 0 and 1 and represents the proportion of the data set
    to use for training. This parameter has a default value of 0.25.
    random_state: Used as a seed to the randomization. This parameter has a default value of 1870.
    Returns the data split into 4 subsets, corresponding to those returned by train_test_split:
    x_train, x_test, y_train, and y_test. where units is the "x" column and the input parameter,
    y_col_name is the "y" column.
    '''
    return train_test_split(df["units"], df[y_col_name], test_size=test_size, \
        random_state=random_state)

def fit_poly(xes, yes, epsilon=100, verbose=False):
    '''
    This function takes four inputs:
    xes: a DataFrame that includes the column units.
    yes: a series of the same length as xes.
    epsilon: the size of the sample. It has a default value of 100.
    verbose: when True, prints out the MSE cost for each degree tried
    (in format: f'MSE cost for deg {deg} poly model: {error:.3f}' for degrees 1, 2, ...,
    until the error is below epsilon, see example below). It has a default value of False.
    It returns the smallest integer degree >= 1 for which the model yields a MSE of less than the
    specified epsilon and the coefficients as a vector for df["units"] and df[y_col]. If it does
    not find a model with an error less than epsilon by degree 5, returns None. When fitting the
    linear regression model, the fit_intercept=False.
    Hint: see the Chapter 16 for examples of using PolynomialFeatures().
    '''
    degree = 1
    while degree < epsilon:
        poly = PolynomialFeatures(degree=degree)
        poly_features = poly.fit_transform(xes[['units']])
        x_train, x_test, y_train, y_test = train_test_split(poly_features,yes)
        model = LinearRegression(fit_intercept=False).fit(x_train,y_train)
        y_pred = model.predict(x_test)
        mse = mean_squared_error(y_pred, y_test)
        if verbose:
            print(f'MSE cost for deg {degree} poly model: {mse:.3f}')
        if mse < epsilon - 5:
            return degree
        degree += 1
    return None




def fit_model(xes, yes, poly_deg=2,reg = "lasso"):
    '''
    xes: a series of numeric values.
    yes: a series of numeric values.
    poly_deg: the degree of the polynomial features to be created. It has a default value of 2.
    reg: The type of regularization used: ridge or lasso. It has a default value of lasso.
    This function fits a model with polynomial features using Lasso or Ridge
    regression with cross validation:
    Apply PolynomialFeatures to the xes with degree equal to poly_deg.
    If reg equals ridge, use RidgeCV to instantiate and fit a model to the polynomial features
    and yes. Otherwise, use LassoCV to to instantiate and fit the model.
    Returns the model as serialized object (i.e. a pickled object)
    '''
    xes
    yes
    poly_deg
    reg

def predict_using_trained_model(mod_pkl, poly_xes, yes):
    '''
    This function takes three inputs:
    mod_pkl: a trained model for the data, stored in pickle format.
    poly_xes: an array or DataFrame of numeric columns with no null values.
    yes: an array or DataFrame of numeric columns with no null values.
    Computes and returns the mean squared error and r2 score between the values predicted by the
    model (mod on x) and the actual values (y). Note that sklearn.metrics contains two functions
    that may be of use: mean_squared_error and r2_score.
    '''
    mod = pickle.loads(mod_pkl)
    y_pred = mod.predict(poly_xes)
    return mean_squared_error(y_pred,yes), r2_score(y_pred, yes)


def main():
    '''
    testing
    '''
    csv_file = "fred_cpi_eggs_oil_cars_INDEXED.csv"
    df = import_data(csv_file)
    #print('The DataFrame:')
    #print(df)
    y_col_name = "CPI"
    #print(f'For the x column = "units", y_col = {y_col_name}')
    #print(df[ ["units", y_col_name] ])
    x_train_cpi, x_test_cpi, y_train_cpi, y_test_cpi = split_data(df, y_col_name)
    #print('\nReturned sets of lengths:')
    #print(f"x_train_cpi: {len(x_train_cpi)}, x_test_cpi: {len(x_test_cpi)}")
    #print(f"y_train_cpi: {len(y_train_cpi)}, y_test_cpi: {len(y_test_cpi)}")
    eps = 5
    print(f'Finding the poly degree for training data with epsilon = {eps}:')
    deg = fit_poly(x_train_cpi.to_frame(),y_train_cpi,epsilon=eps,verbose=True)
    print(f'For epsilon = {eps}, poly has degree {deg}.')

if __name__ == "__main__":
    main()
